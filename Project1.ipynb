{
 "metadata": {
  "name": "",
  "signature": "sha256:4c4775fa276a844e231bef90cb9330994e3f120f3de142c04c587fcab336bb2d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Before we get started...\n",
      "We are going to be doing some visualizations later on. I would like to see them embedded in the notebook, so I'm going to run: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "%pylab inline\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## So, what kind of data are we looking at?\n",
      "I was originally thinking about doing something with **Spotify** user data, but then realized would have to set up some sort of crawler that would have to continually hit the API if I wanted to get enough of this data. Short on time and lacking experience with APIs, I figured it might be easier for me to look at some kind of music data that has already been aggregated, and then maybe look for Spotify user data later on if this data isn't enough. This is how I came across the [Million Song Dataset](http://labrosa.ee.columbia.edu/millionsong/).\n",
      "\n",
      "### What is the Million Song Dataset?\n",
      "From the site: *The Million Song Dataset is a freely-available collection of audio features and metadata for a million contemporary popular music tracks.* \n",
      "\n",
      "For every song in the dataset, there is [a bunch of interesting metadata about the song](http://labrosa.ee.columbia.edu/millionsong/pages/example-track-description).\n",
      "\n",
      "(yes, you just got metadata [rick-rolled](https://www.youtube.com/watch?v=dQw4w9WgXcQ))\n",
      "\n",
      "### Isn't a million songs a lot of data?\n",
      "Yes. The full set is 300 GB. Luckily, there is a 10,000 song subset available, which is what I'm going to be exploring in this notebook.\n",
      "\n",
      "\n",
      "In addition to the main song info, the project also has several complementary datasets that were interesting to me. I'm interested in lyrics data in particular which is available [here](http://labrosa.ee.columbia.edu/millionsong/musixmatch).\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Getting and Loading the Million Song Subset Data \n",
      "\n",
      "I downloaded the subset [here](http://labrosa.ee.columbia.edu/millionsong/pages/getting-dataset#subset) and put it inside a Songs folder that I created in my ga_assignments directory.\n",
      "\n",
      "### Where am I now?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pwd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "u'/home/sally/projects/ga_assignments'"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Where are all the songs?\n",
      "Wouldn't it be so nice if they were all stored in one folder? They're not. There are many subsets to the subset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "data_dir = 'Songs/MillionSongSubset/data/'\n",
      "dirs = os.listdir(data_dir)\n",
      "print \"Subsets in the 'data' folder:\", dirs # there are two folders in the \"data\" folder\n",
      "\n",
      "data_sets = {}\n",
      "for dir in dirs:\n",
      "    data_sets[data_dir + dir] = os.listdir(data_dir + dir)\n",
      "print \"Subsets in each subset folder:\",data_sets  #printing a dictionary of the folders in each folder"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Subsets in the 'data' folder: ['A', 'B']\n",
        "Subsets in each subset folder: {'Songs/MillionSongSubset/data/B': ['E', 'A', 'F', 'D', 'H', 'I', 'B', 'G', 'C'], 'Songs/MillionSongSubset/data/A': ['E', 'A', 'F', 'D', 'O', 'R', 'M', 'Q', 'N', 'L', 'S', 'K', 'H', 'J', 'I', 'T', 'W', 'Z', 'P', 'B', 'G', 'Y', 'C', 'U', 'X', 'V']}\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Ok... the songs?\n",
      "Still haven't found the songs yet! We have to go through all these folders to find them, and then get their file paths before we can parse the contents of the song files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_paths_all = []\n",
      "for key, vals in data_sets.iteritems():\n",
      "    for val in vals:\n",
      "        for folder in os.listdir(key + '/' + val):\n",
      "            path = key + '/' + val + '/' + folder\n",
      "            file_paths = []\n",
      "            for f in os.listdir(path):\n",
      "                file_paths.append(path + '/' + f)\n",
      "                file_paths_all.append(path + '/' + f)\n",
      "print \"Number of songs in MSD subset for analysis:\",len(file_paths_all)\n",
      "print \"Some example file paths:\", file_paths_all[0:2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of songs in MSD subset for analysis: 10000\n",
        "Some example file paths: ['Songs/MillionSongSubset/data/B/E/E/TRBEEEP128F426C17D.h5', 'Songs/MillionSongSubset/data/B/E/E/TRBEEQG128F42732AB.h5']\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great, it looks like we have our 10k song files and their file paths. Now how do we look at their data?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## What on earth is a .h5 file and how do I parse one?\n",
      "\n",
      "All of the song info is stored in an .h5 format. I have never seen one of these before! I have no idea what to do with this. \n",
      "\n",
      "Luckily, the people who created the dataset have provided some code for parsing these out [here](https://github.com/tbertinmahieux/MSongsDB). I downloaded what they had and will be using their python code. \n",
      "\n",
      "I put this MSongsDB folder in the Songs folder I created earlier. Let's import the hdf5_getters module and see what kind of data we can get."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd 'Songs/MSongsDB/PythonSrc/'\n",
      "import hdf5_getters as getters\n",
      "print help(getters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/sally/projects/ga_assignments/Songs/MSongsDB/PythonSrc\n",
        "Help on module hdf5_getters:\n",
        "\n",
        "NAME\n",
        "    hdf5_getters\n",
        "\n",
        "FILE\n",
        "    /home/sally/projects/ga_assignments/Songs/MSongsDB/PythonSrc/hdf5_getters.py\n",
        "\n",
        "DESCRIPTION\n",
        "    Thierry Bertin-Mahieux (2010) Columbia University\n",
        "    tb2332@columbia.edu\n",
        "    \n",
        "    \n",
        "    This code contains a set of getters functions to access the fields\n",
        "    from an HDF5 song file (regular file with one song or\n",
        "    aggregate / summary file with many songs)\n",
        "    \n",
        "    This is part of the Million Song Dataset project from\n",
        "    LabROSA (Columbia University) and The Echo Nest.\n",
        "    \n",
        "    \n",
        "    Copyright 2010, Thierry Bertin-Mahieux\n",
        "    \n",
        "    This program is free software: you can redistribute it and/or modify\n",
        "    it under the terms of the GNU General Public License as published by\n",
        "    the Free Software Foundation, either version 3 of the License, or\n",
        "    (at your option) any later version.\n",
        "    \n",
        "    This program is distributed in the hope that it will be useful,\n",
        "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "    GNU General Public License for more details.\n",
        "    \n",
        "    You should have received a copy of the GNU General Public License\n",
        "    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
        "\n",
        "FUNCTIONS\n",
        "    get_analysis_sample_rate(h5, songidx=0)\n",
        "        Get analysis sample rate from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_artist_7digitalid(h5, songidx=0)\n",
        "        Get artist 7digital id from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_artist_familiarity(h5, songidx=0)\n",
        "        Get artist familiarity from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_artist_hotttnesss(h5, songidx=0)\n",
        "        Get artist hotttnesss from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_artist_id(h5, songidx=0)\n",
        "        Get artist id from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_artist_latitude(h5, songidx=0)\n",
        "        Get artist latitude from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_artist_location(h5, songidx=0)\n",
        "        Get artist location from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_artist_longitude(h5, songidx=0)\n",
        "        Get artist longitude from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_artist_mbid(h5, songidx=0)\n",
        "        Get artist musibrainz id from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_artist_mbtags(h5, songidx=0)\n",
        "        Get artist musicbrainz tag array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_artist_mbtags_count(h5, songidx=0)\n",
        "        Get artist musicbrainz tag count array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_artist_name(h5, songidx=0)\n",
        "        Get artist name from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_artist_playmeid(h5, songidx=0)\n",
        "        Get artist playme id from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_artist_terms(h5, songidx=0)\n",
        "        Get artist terms array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_artist_terms_freq(h5, songidx=0)\n",
        "        Get artist terms array frequencies. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_artist_terms_weight(h5, songidx=0)\n",
        "        Get artist terms array frequencies. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_audio_md5(h5, songidx=0)\n",
        "        Get audio MD5 from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_bars_confidence(h5, songidx=0)\n",
        "        Get bars start array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_bars_start(h5, songidx=0)\n",
        "        Get bars start array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_beats_confidence(h5, songidx=0)\n",
        "        Get beats confidence array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_beats_start(h5, songidx=0)\n",
        "        Get beats start array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_danceability(h5, songidx=0)\n",
        "        Get danceability from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_duration(h5, songidx=0)\n",
        "        Get duration from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_end_of_fade_in(h5, songidx=0)\n",
        "        Get end of fade in from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_energy(h5, songidx=0)\n",
        "        Get energy from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_key(h5, songidx=0)\n",
        "        Get key from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_key_confidence(h5, songidx=0)\n",
        "        Get key confidence from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_loudness(h5, songidx=0)\n",
        "        Get loudness from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_mode(h5, songidx=0)\n",
        "        Get mode from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_mode_confidence(h5, songidx=0)\n",
        "        Get mode confidence from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_num_songs(h5)\n",
        "        Return the number of songs contained in this h5 file, i.e. the number of rows\n",
        "        for all basic informations like name, artist, ...\n",
        "    \n",
        "    get_release(h5, songidx=0)\n",
        "        Get release from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_release_7digitalid(h5, songidx=0)\n",
        "        Get release 7digital id from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_sections_confidence(h5, songidx=0)\n",
        "        Get sections confidence array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_sections_start(h5, songidx=0)\n",
        "        Get sections start array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_segments_confidence(h5, songidx=0)\n",
        "        Get segments confidence array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_segments_loudness_max(h5, songidx=0)\n",
        "        Get segments loudness max array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_segments_loudness_max_time(h5, songidx=0)\n",
        "        Get segments loudness max time array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_segments_loudness_start(h5, songidx=0)\n",
        "        Get segments loudness start array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_segments_pitches(h5, songidx=0)\n",
        "        Get segments pitches array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_segments_start(h5, songidx=0)\n",
        "        Get segments start array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_segments_timbre(h5, songidx=0)\n",
        "        Get segments timbre array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_similar_artists(h5, songidx=0)\n",
        "        Get similar artists array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_song_hotttnesss(h5, songidx=0)\n",
        "        Get song hotttnesss from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_song_id(h5, songidx=0)\n",
        "        Get song id from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_start_of_fade_out(h5, songidx=0)\n",
        "        Get start of fade out from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_tatums_confidence(h5, songidx=0)\n",
        "        Get tatums confidence array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_tatums_start(h5, songidx=0)\n",
        "        Get tatums start array. Takes care of the proper indexing if we are in aggregate\n",
        "        file. By default, return the array for the first song in the h5 file.\n",
        "        To get a regular numpy ndarray, cast the result to: numpy.array( )\n",
        "    \n",
        "    get_tempo(h5, songidx=0)\n",
        "        Get tempo from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_time_signature(h5, songidx=0)\n",
        "        Get signature from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_time_signature_confidence(h5, songidx=0)\n",
        "        Get signature confidence from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_title(h5, songidx=0)\n",
        "        Get title from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_track_7digitalid(h5, songidx=0)\n",
        "        Get track 7digital id from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_track_id(h5, songidx=0)\n",
        "        Get track id from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    get_year(h5, songidx=0)\n",
        "        Get release year from a HDF5 song file, by default the first song in it\n",
        "    \n",
        "    open_h5_file_read(h5filename)\n",
        "        Open an existing H5 in read mode.\n",
        "        Same function as in hdf5_utils, here so we avoid one import\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's a lot of metadata!\n",
      "\n",
      "### Let's parse through all the song files (.h5 files) to get the info we're interested in.\n",
      "\n",
      "First we will go back to our main assignments folder so that we can read every file from our list of file paths (`file_paths_all`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd '~/projects/ga_assignments/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/sally/projects/ga_assignments\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we will run open every file (using the `getters.open_h5_file_read` method) and run the getters for every datapoint we want to look at. We will put this data in a pandas dataframe."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "i = 1 # The number of the song we're parsing\n",
      "df = pd.DataFrame(columns=('Artist', 'Title', 'Year'))\n",
      "\n",
      "for h5_file_name in file_paths_all:\n",
      "    h5 = getters.open_h5_file_read(h5_file_name)\n",
      "    if h5.root:\n",
      "        try:\n",
      "            print i, getters.get_artist_name(h5),\",\", getters.get_title(h5), \",\", getters.get_year(h5), \",\", getters.get_track_id(h5) \n",
      "            i+=1\n",
      "            if i > 100:\n",
      "                break\n",
      "        except:\n",
      "            print \"something is up with\", h5_file_name\n",
      "    h5.close()    \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Songs/MillionSongSubset/data/B/E/E/TRBEEEP128F426C17D.h5\n",
        "1 A Day To Remember , The Plot To Bomb The Panhandle (Album Version) , 0 , TRBEEEP128F426C17D\n",
        "2 Henri Pousseur , Crosses of crossed colors 3 , 0 , TRBEEQG128F42732AB\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Santana , Fried Neck Bones And Some Home Fries , 0 , TRBEEQF128F4283395\n",
        "4 Namatjira , 1000 Of Years Ago , 0 , TRBEEJH128F92FAC0D\n",
        "5 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P.O.D. , Eyes Of A Stranger (Album Version) , 0 , TRBEERY128F4231A2E\n",
        "6 Leon Lai , Qing Yi Shi , 0 , TRBEEWC128F9344A22\n",
        "7 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Arc Angels , Sent By Angels , 1992 , TRBEELS128F42B63A8\n",
        "8 Jack Orsen feat. Fumanschu_ Justus , Getto Boys , 2004 , TRBEEWS12903CD0B60\n",
        "9 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fall Out Boy , Thriller , 2007 , TRBEEDK128F424D995\n",
        "10 Alfredo Guti\u00e9rrez , El troyano , 0 , TRBEEAF128F92E6382\n",
        "11 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Handsome Boy Modeling School , Megaton B-Boy 2000 (LP Version) , 0 , TRBEEJS128F9340A8B\n",
        "12 Jesse Malin , Wendy , 2002 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBEEMB128F9320082\n",
        "13 Maria & Margot Hellwig/Die Garmischer Alpenj\u00e4ger , Lieder_ Die Uns Ein Leben Lang Begleiten , 0 , TRBEAXM12903CC63F5\n",
        "14 Frank Ifield , Yodelling Mad (2002 Digital Remaster) , 0 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBEADA128F1455D45\n",
        "15 Vincenzo , Valium , 0 , TRBEARH128F422A535\n",
        "16 Christina Aguilera , Cruz , 2002 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBEAMO128F425B75F\n",
        "17 Melanie And The Secret Army , All The Lessons , 0 , TRBEAYH12903CBC75F\n",
        "18 Simply Red , So Not Over You (Motivo-Pop Lectro Mix) , 2007 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBEACH128EF341C45\n",
        "19 Las Ni\u00f1as , Castillos Y Sue\u00f1os , 2005 , TRBEAOU128E0798350\n",
        "20 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mix Mob , Show Me The Way , 0 , TRBEAJB128F4214A06\n",
        "21 Triana Pura , Afonsina y el mar , 0 , TRBEATF128F9309E63\n",
        "22 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Frost , Take a Ride , 0 , TRBEAXZ128F424347B\n",
        "23 Bon Jovi , Only Lonely , 1985 , TRBEATE128E078F0D8\n",
        "24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Cali , C'est Toujours Le Matin , 2003 , TRBEAZN128F14858B3\n",
        "25 Casiotone For The Painfully Alone , Happy Mother's Day , 2006 , TRBEAWP128F422B4D1\n",
        "26 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Alexandre Desplat , Meeting Daisy , 2008 , TRBEFQY128F932EE3F\n",
        "27 Davol , Another Land (World) , 0 , TRBEFKV128F930114A\n",
        "28 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "KALIMBA , No Volveras A Mi , 2007 , TRBEFFA128F930F017\n",
        "29 Sensations , Superscout , 2005 , TRBEFAS128F4262569\n",
        "30 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monster Magnet , Black Celebration , 1998 , TRBEFFG128E0793C24\n",
        "31 Eleanor McEvoy , Sophie , 0 , TRBEFJE128F932139E\n",
        "32 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dame Edna Everage with Carl Davis conducting the London Symphony Orchestra and The New Antipodean Singers , Song Of Australia - Canto 6 (2009 Digital Remaster) , 0 , TRBEFUY12903C98485\n",
        "33 Gisele MacKenzie , Le Fiacre , 0 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBEFDI12903CCF154\n",
        "34 Eleanor McEvoy , Please Heart_ You're Killing Me , 0 , TRBEFQB128F932138A\n",
        "35 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Despina Vandi , Ke T' Oniro Egine Efialtis , 0 , TRBEFWM128F428EFA8\n",
        "36 White Heart , Change The Way (Highlands Album Version) , 0 , TRBEDZL128F425A2E7\n",
        "37 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tri Atma , RAMUKA , 0 , TRBEDXK128F14A7EC0\n",
        "38 D.A.D. , Girl Nation [Live] , 0 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBEDHB12903CDB820\n",
        "39 Alias , What To Do , 0 , TRBEDBC128F145B134\n",
        "40 Savoy Brown , All Burned Out , 1989 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBEDRW128F425F424\n",
        "41 Frank Vignola , But Not For Me , 0 , TRBEDIR128F4250FBD\n",
        "42 The Cat's Miaow , You Know It's True , 1997 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBEDLE128F933039C\n",
        "43 Angela Dimitriou , Edo Pou Ta Leme , 0 , TRBEDTM128F4255119\n",
        "44 Shaggy / Brian & Tony Gold , Hey Sexy Lady , 2002 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBEDVH128F14599C7\n",
        "45 Marcel Mouloudji , Madeleine , 0 , TRBEOIU128F4296D36\n",
        "46 The Blind Boys Of Alabama , New Born Soul , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2005 , TRBEONX128E0788D2B\n",
        "47 Vex'd , Oceans , 2010 , TRBEOUT12903CD5981\n",
        "48 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Princess Lover , Prends-Moi La Main , 2007 , TRBEORE128F92E8476\n",
        "49 Blind Blake , Too Tight Blues No. 2 , 0 , TRBEODT128F424AE26\n",
        "50 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Brazilian Tropical Orchestra , If You Leave Me Now , 0 , TRBEORW128F42A3A56\n",
        "51 Al Di Meola , Splendido Sundance , 1980 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBEOCF128F426BFDC\n",
        "52 System of a Down , War? , 1996 , TRBEOSC128F426AB23\n",
        "53 Antigone Rising , Don't Look Back (Radio Edit) , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2005 , TRBEOYQ128F42955C3\n",
        "54 Sunidhi Chauhan / Anu Malik / Jatin Sharma , Dekh Le , 0 , TRBEONB128EF3405E6\n",
        "55 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Joe Bonamassa , So Many Roads , 2006 , TRBEOUP12903CF652E\n",
        "56 Kieran , You Saved My Life , 0 , TRBEOMZ12903CBCD33\n",
        "57 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Seguridad Social , El Oro De La Gallina De Los Huevos , 1999 , TRBEOOD128E0795C84\n",
        "58 Egberto Gismonti , Baiao Malandro , 0 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBERUM128F148AC93\n",
        "59 Los Traileros Del Norte , Perro Faldero , 0 , TRBERHS128F92FF079\n",
        "60 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Stefano Battaglia_ Giovanni Maier_ Michele Rabbia , Raccolto , 0 , TRBERKY128F42290CA\n",
        "61 Massive T\u00f6ne , Im Club , 2002 , TRBEROX128F4270745\n",
        "62 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Winston McAnuff_ The Bazbaz Orchestra , Common sense , 2005 , TRBERHG128F933AC46\n",
        "63 Caramell , Expolodera (som dynamit) , 1999 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBERCZ128E0787D93\n",
        "64 Gary Moore , I Can't Wait Until Tomorrow (Live) , 0 , TRBERJV128F1458463\n",
        "65 Frank Stokes , Sweet To Mama , 2005 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBERGT128F42A009F\n",
        "66 Bela Fleck And The Flecktones , Throwdown At The Hoedown (LP Version) , 0 , TRBERIX12903CEF907\n",
        "67 Zombina & The Skeletones , I Was a Human Bomb for the FBI , 2006 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBERLZ12903CC4747\n",
        "68 Cedric Myton , Wisdom , 0 , TRBERJE128F425F098\n",
        "69 Jake Hess , I Just Love Old People (Rocky Mountain Homecoming Version) , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 , TRBEMRD128F426DEC7\n",
        "70 Leroy Huston , Love Oh Love (Live in Chicago) , 0 , TRBEMAH128F424215B\n",
        "71 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Alan Silvestri , We're Out Of Gas , 1990 , TRBEMTB128F14ACBDF\n",
        "72 Karen Peck And New River , Four Days Late (Studio Track w/o Background Vocals) , 0 , TRBEMBM128F9324F2B\n",
        "73 SNAP! , The World In My Hands (We Are One) , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1994 , TRBEMJC128F14A6E0D\n",
        "74 Bobby Broom , MONDAY_ MONDAY , 0 , TRBEMMZ128F145C514\n",
        "75 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Joy Kitikonti , Pleasure Zone Mix , 0 , TRBEMDL128F424ABD6\n",
        "76 For Squirrels , Superstar , 1995 , TRBEMTF128F42660CA\n",
        "77 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Klaus Badelt , Moonlight Serenade , 2003 , TRBEQFT128F146BD6D\n",
        "78 Aesop Rock , N.Y. Electric / Hunter Interlude , 2003 , TRBEQZJ128F9300B50\n",
        "79 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tony Tuff , Say Something , 0 , TRBEQUV12903CBD17A\n",
        "80 T.O.K. , Chi Chi Man , 2001 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBEQFT128F4238A28\n",
        "81 Serpentine , Passion_ Love & Pain (Feat. 25 Kids) , 0 , TRBEQNH12903CB087E\n",
        "82 Aleks Syntek , In My Arms (Live Auditorio Nacional 08) , 0 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBEQPB128F933D621\n",
        "83 Jimmie Vaughan , Kinky Woman , 0 , TRBEQNB128F9327242\n",
        "84 Marvin Hamlisch;John Lithgow;Jeffrey Huard , At the Fountain (Reprise) (Brian d'Arcy James & Ensemble) , 0 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBEQHL128F4272CBC\n",
        "85 The Nelons , God Keeps A Candle , 0 , TRBENOX128F4243501\n",
        "86 Taboo , Gyere velem , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 , TRBENCK128F4232BB2\n",
        "87 Pink Floyd , The Gold It's In The ... (1996 Digital Remaster) , 1972 , TRBENTT128E0781C26\n",
        "88 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Carl Smith , I Wonder Where You Are Tonight , 0 , TRBENNA12903D0411A\n",
        "89 Devo , Be Stiff (Live) , 1993 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBENDH128F1467D62\n",
        "90 Frl.Menke , Komm Computer , 1982 , TRBENHU12903CB6830\n",
        "91 Blind Blake , Bootleg Rum Dum Blues , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 , TRBENWE128F424AE09\n",
        "92 The Hollywood Brats , Empty Bottles , 0 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBENIG128F92F281E\n",
        "93 Roger Miller , Husbands And Wives , 0 , TRBENZL128F1455ECA\n",
        "94 Chris Farlowe , Blues As Blues Can Get , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 , TRBENBG12903CF94B4\n",
        "95 Prince , She Spoke 2 Me ( Extended Remix LP Version) , 0 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBENZB128F429C5FD\n",
        "96 El Tiempo , Tu Ex Amor , 0 , TRBENZA12903CBF40F\n",
        "97"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Ayo , Girls , 0 , TRBELAZ128F422B87F\n",
        "98 Parallel Sound , Around The World , 0 , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TRBELVP128F93263B8\n",
        "99 Jean Segurel , La Saint-Jean d'\u00e9t\u00e9 , 0 , TRBELXO128F427CAFA\n",
        "100 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mark Lowry , Fritzy & Helen Hanft (Album) , 0 , TRBELUA12903CB0371\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Read in Lyrics Database\n",
      "I'm not sure what this database contains, so I'm going to explore it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pwd\n",
      "import sqlite3\n",
      "conn = sqlite3.connect(\"Songs/Lyrics/mxm_dataset.db\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Explore the Database\n",
      "What tables, columns and values do I have?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = conn.execute(\"SELECT * FROM sqlite_master WHERE type='table'\")\n",
      "tables = res.fetchall()\n",
      "for table in tables:\n",
      "    print table\n",
      "    res = conn.execute(\"SELECT COUNT (*) FROM \" + table[2])\n",
      "    print \"# of rows in\", table[2], \":\", res.fetchall()\n",
      "    print \"First 50 rows:\"\n",
      "    res = conn.execute(\"SELECT * FROM \" + table[2] + \" LIMIT 50\")\n",
      "    print res.fetchall()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'table', u'words', u'words', 2, u'CREATE TABLE words (word TEXT PRIMARY KEY)')\n",
        "# of rows in words : [(5000,)]\n",
        "First 50 rows:\n",
        "[(u'i',), (u'the',), (u'you',), (u'to',), (u'and',), (u'a',), (u'me',), (u'it',), (u'not',), (u'in',), (u'my',), (u'is',), (u'of',), (u'your',), (u'that',), (u'do',), (u'on',), (u'are',), (u'we',), (u'am',), (u'will',), (u'all',), (u'for',), (u'no',), (u'be',), (u'have',), (u'love',), (u'so',), (u'know',), (u'this',), (u'but',), (u'with',), (u'what',), (u'just',), (u'when',), (u'like',), (u'now',), (u'que',), (u'time',), (u'can',), (u'come',), (u'de',), (u'there',), (u'go',), (u'up',), (u'oh',), (u'la',), (u'one',), (u'they',), (u'out',)]\n",
        "(u'table', u'lyrics', u'lyrics', 4, u'CREATE TABLE lyrics (track_id, mxm_tid INT, word TEXT, count INT, is_test INT, FOREIGN KEY(word) REFERENCES words(word))')\n",
        "# of rows in"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " lyrics : [(19045332,)]\n",
        "First 50 rows:\n",
        "[(u'TRAAAAV128F421A322', 4623710, u'i', 6, 0), (u'TRAAAAV128F421A322', 4623710, u'the', 4, 0), (u'TRAAAAV128F421A322', 4623710, u'you', 2, 0), (u'TRAAAAV128F421A322', 4623710, u'to', 2, 0), (u'TRAAAAV128F421A322', 4623710, u'and', 5, 0), (u'TRAAAAV128F421A322', 4623710, u'a', 3, 0), (u'TRAAAAV128F421A322', 4623710, u'me', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'it', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'my', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'is', 2, 0), (u'TRAAAAV128F421A322', 4623710, u'of', 3, 0), (u'TRAAAAV128F421A322', 4623710, u'your', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'that', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'are', 2, 0), (u'TRAAAAV128F421A322', 4623710, u'we', 2, 0), (u'TRAAAAV128F421A322', 4623710, u'am', 2, 0), (u'TRAAAAV128F421A322', 4623710, u'will', 2, 0), (u'TRAAAAV128F421A322', 4623710, u'for', 4, 0), (u'TRAAAAV128F421A322', 4623710, u'be', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'have', 2, 0), (u'TRAAAAV128F421A322', 4623710, u'so', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'this', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'like', 2, 0), (u'TRAAAAV128F421A322', 4623710, u'de', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'up', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'was', 2, 0), (u'TRAAAAV128F421A322', 4623710, u'if', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'got', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'would', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'been', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'these', 2, 0), (u'TRAAAAV128F421A322', 4623710, u'seem', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'someon', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'understand', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'pass', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'river', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'met', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'piec', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'damn', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'worth', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'flesh', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'grace', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'poor', 2, 0), (u'TRAAAAV128F421A322', 4623710, u'somehow', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'ignor', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'passion', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'tide', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'season', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'seed', 1, 0), (u'TRAAAAV128F421A322', 4623710, u'resist', 1, 0)]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "dataframes = []\n",
      "for table in tables:\n",
      "    dataframes.append(pd.read_sql(\"SELECT * from \" + table[2] + \" LIMIT 10000\" , conn))\n",
      "words_df, lyrics_df = dataframes \n",
      "lyrics_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>track_id</th>\n",
        "      <th>mxm_tid</th>\n",
        "      <th>word</th>\n",
        "      <th>count</th>\n",
        "      <th>is_test</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> TRAAAAV128F421A322</td>\n",
        "      <td> 4623710</td>\n",
        "      <td>   i</td>\n",
        "      <td> 6</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> TRAAAAV128F421A322</td>\n",
        "      <td> 4623710</td>\n",
        "      <td> the</td>\n",
        "      <td> 4</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> TRAAAAV128F421A322</td>\n",
        "      <td> 4623710</td>\n",
        "      <td> you</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> TRAAAAV128F421A322</td>\n",
        "      <td> 4623710</td>\n",
        "      <td>  to</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> TRAAAAV128F421A322</td>\n",
        "      <td> 4623710</td>\n",
        "      <td> and</td>\n",
        "      <td> 5</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "             track_id  mxm_tid word  count  is_test\n",
        "0  TRAAAAV128F421A322  4623710    i      6        0\n",
        "1  TRAAAAV128F421A322  4623710  the      4        0\n",
        "2  TRAAAAV128F421A322  4623710  you      2        0\n",
        "3  TRAAAAV128F421A322  4623710   to      2        0\n",
        "4  TRAAAAV128F421A322  4623710  and      5        0"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_groupby = dataframes[1].groupby(['track_id','word'])\n",
      "word_groupby.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th>mxm_tid</th>\n",
        "      <th>count</th>\n",
        "      <th>is_test</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>track_id</th>\n",
        "      <th>word</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th rowspan=\"30\" valign=\"top\">TRAAAAV128F421A322</th>\n",
        "      <th>a</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  3</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>am</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>and</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  5</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>are</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>arrang</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>be</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>been</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>captur</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>damn</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>de</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>devast</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>element</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>fashion</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>flesh</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>for</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  4</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>got</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>grace</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>grant</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>have</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>highest</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>i</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  6</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ici</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>if</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ignor</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>is</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>it</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>leaf</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>lifeless</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>like</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>me</th>\n",
        "      <td>  4623710</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>...</th>\n",
        "      <th>...</th>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th rowspan=\"30\" valign=\"top\">TRAAIBF128F4282493</th>\n",
        "      <th>she</th>\n",
        "      <td> 13169255</td>\n",
        "      <td> 20</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>show</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>so</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stop</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>that</th>\n",
        "      <td> 13169255</td>\n",
        "      <td> 16</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>the</th>\n",
        "      <td> 13169255</td>\n",
        "      <td> 35</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>them</th>\n",
        "      <td> 13169255</td>\n",
        "      <td> 10</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>they</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  3</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>thing</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  8</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>think</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  3</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>this</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>to</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  5</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>told</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>took</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>turn</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  4</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>two</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>up</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>wait</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>want</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>was</th>\n",
        "      <td> 13169255</td>\n",
        "      <td> 11</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>way</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>what</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  4</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>when</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>where</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>will</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  3</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>with</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  8</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>woman</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>work</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>would</th>\n",
        "      <td> 13169255</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>you</th>\n",
        "      <td> 13169255</td>\n",
        "      <td> 11</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>10000 rows \u00d7 3 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "                              mxm_tid  count  is_test\n",
        "track_id           word                              \n",
        "TRAAAAV128F421A322 a          4623710      3        0\n",
        "                   am         4623710      2        0\n",
        "                   and        4623710      5        0\n",
        "                   are        4623710      2        0\n",
        "                   arrang     4623710      1        0\n",
        "                   be         4623710      1        0\n",
        "                   been       4623710      1        0\n",
        "                   captur     4623710      2        0\n",
        "                   damn       4623710      1        0\n",
        "                   de         4623710      1        0\n",
        "                   devast     4623710      1        0\n",
        "                   element    4623710      1        0\n",
        "                   fashion    4623710      1        0\n",
        "                   flesh      4623710      1        0\n",
        "                   for        4623710      4        0\n",
        "                   got        4623710      1        0\n",
        "                   grace      4623710      1        0\n",
        "                   grant      4623710      1        0\n",
        "                   have       4623710      2        0\n",
        "                   highest    4623710      2        0\n",
        "                   i          4623710      6        0\n",
        "                   ici        4623710      1        0\n",
        "                   if         4623710      1        0\n",
        "                   ignor      4623710      1        0\n",
        "                   is         4623710      2        0\n",
        "                   it         4623710      1        0\n",
        "                   leaf       4623710      1        0\n",
        "                   lifeless   4623710      1        0\n",
        "                   like       4623710      2        0\n",
        "                   me         4623710      1        0\n",
        "...                               ...    ...      ...\n",
        "TRAAIBF128F4282493 she       13169255     20        0\n",
        "                   show      13169255      2        0\n",
        "                   so        13169255      2        0\n",
        "                   stop      13169255      1        0\n",
        "                   that      13169255     16        0\n",
        "                   the       13169255     35        0\n",
        "                   them      13169255     10        0\n",
        "                   they      13169255      3        0\n",
        "                   thing     13169255      8        0\n",
        "                   think     13169255      3        0\n",
        "                   this      13169255      1        0\n",
        "                   to        13169255      5        0\n",
        "                   told      13169255      1        0\n",
        "                   took      13169255      1        0\n",
        "                   turn      13169255      4        0\n",
        "                   two       13169255      1        0\n",
        "                   up        13169255      2        0\n",
        "                   wait      13169255      1        0\n",
        "                   want      13169255      2        0\n",
        "                   was       13169255     11        0\n",
        "                   way       13169255      1        0\n",
        "                   what      13169255      4        0\n",
        "                   when      13169255      1        0\n",
        "                   where     13169255      1        0\n",
        "                   will      13169255      3        0\n",
        "                   with      13169255      8        0\n",
        "                   woman     13169255      1        0\n",
        "                   work      13169255      1        0\n",
        "                   would     13169255      1        0\n",
        "                   you       13169255     11        0\n",
        "\n",
        "[10000 rows x 3 columns]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Create some Non-categorical Variables\n",
      "* Good for linear regression"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}